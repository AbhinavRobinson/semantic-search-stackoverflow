# -*- coding: utf-8 -*-
"""testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WrwN-xT_xhB5qZHrik9jtBSmwOx2VPNx
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import nltk
import requests
from itertools import combinations
tf.logging.set_verbosity(tf.logging.ERROR)

def get_similarity(messages):
    # Downloading the pre-trained "Universal Sentence Encoder" from tensorflow hub
    module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" 
    embed = hub.Module(module_url)
    similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))
    similarity_message_encodings = embed(similarity_input_placeholder)
    with tf.Session() as session:
        session.run(tf.global_variables_initializer())
        session.run(tf.tables_initializer())
        message_embeddings = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})
        per = np.inner(message_embeddings, message_embeddings[-1:])

    # Adding probabilities to title
    dictItems = []
    i = 0
    for i in range(0, len(per)-1 ):
        temp = { "probability" : per.item(i), "title" : messages[i] }
        dictItems.append(temp)
    return dictItems

input = 'node  how to run app.js'
# input ='difference between plotly and seaborn'
#Downloading the required libraries for nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
tokenized_word=word_tokenize(input)
stop_words=set(stopwords.words("english"))
filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)
filtered_sent

arr2 = []
for i in filtered_sent:
  arr1 = []
  URL = f'https://api.stackexchange.com/2.2/tags?order=desc&sort=popular&inname={i}&site=stackoverflow'
  r = requests.get(url = URL)
  data = r.json()
  if len(data['items']) != 0:    
    arr1.append(i)
    arr1.append(data['items'][0]['name'])
    t = get_similarity(arr1)
    if t[0]['probability'] > 0.7:  
      arr2.append(data['items'][0]['name'])

poi = {
  "items": [],
  "has_more": False,
  "quota_max": 10000,
  "quota_remaining": 9999
}
len(poi['items'])

arr2