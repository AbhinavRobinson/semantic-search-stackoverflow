{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSW72wUFi5hd",
        "colab_type": "text"
      },
      "source": [
        "# StackOverflow Search Optimazation\n",
        "Click [here](https://colab.research.google.com/drive/1Fbyg6qPFc-sJoK9bc35IO-XpDIt_6zZt) to run this code in Colaboratory. It only takes a minute!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkP906gSi5hf",
        "colab_type": "text"
      },
      "source": [
        "#### Installing the required libraries (the following lines will work only if this code is viewed in jupyter notebook or similar environments)\n",
        "* If you wish to run this in other environments, you have to install these libraries manually.\n",
        "* Tensorflow v1.13.1 is recommended but lower versions should also work fine. \n",
        "* Not compatible with Tensorflow v2.0 out of the box. (You might want to convert to Tensorflow v2.0 format)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ivMWyFYi5hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --quiet tensorflow==1.13.1\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet numpy\n",
        "!pip install --quiet nltk\n",
        "!pip install --quiet requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iK9CeBRi5hm",
        "colab_type": "text"
      },
      "source": [
        "#### Importing the required libraries\n",
        "1. `tensorflow`: the main Machine Learning library for this project.\n",
        "2. `tensorflow_hub`: it provides with the universal sentence encoder needed to calculate the embeddings.\n",
        "3. `numpy`: for matrix operations.\n",
        "4. `requests`: to make API calls.\n",
        "5. `itertools`: we use the `combinations` function calculate combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n4NDfFcheOu0",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import nltk\n",
        "import requests\n",
        "from urllib.parse import quote\n",
        "from urllib.parse import unquote\n",
        "from itertools import combinations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im9NoIuKi5hz",
        "colab_type": "text"
      },
      "source": [
        "#### Processing the input query to get tags\n",
        "1. NLTK is used to tokenize the sentence.\n",
        "2. Then stopwords and punctuations are removed from the tokenized sentence to get `crude_tags`.\n",
        "3. API call is made with the `crude_tags` to get the tags from StackOverflow matching the `crude_tags`.\n",
        "4. Maximum of 5 tags are returned as it is the limit for the API `api_tags` are returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lkG4-QnazgE9",
        "colab": {}
      },
      "source": [
        "#Cleaning the input to get tags\n",
        "def get_tags(query):\n",
        "  #Downloading the required libraries for nltk\n",
        "  nltk.download('stopwords')\n",
        "  from nltk.corpus import stopwords\n",
        "  \n",
        "  tokenized_sentence = query.split(' ')\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  crude_tags = []\n",
        "  for w in tokenized_sentence:\n",
        "      if w not in stop_words:\n",
        "          crude_tags.append(w)\n",
        "  tags = list(set(crude_tags))\n",
        "  \n",
        "  #searching the API for tags using the obtained tags\n",
        "  api_tags = []\n",
        "  for tag in tags:\n",
        "    URL = f'https://api.stackexchange.com/2.2/tags?order=desc&sort=popular&inname={quote(tag)}&site=stackoverflow'\n",
        "    r = requests.get(url = URL)\n",
        "    data = r.json()\n",
        "    if len(data['items']) > 0:\n",
        "      api_tags.append(data['items'][0]['name'])\n",
        "\n",
        "  if len(api_tags) > 5:\n",
        "    return api_tags[:5]\n",
        "  else:\n",
        "    return api_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWuolyJ0i5h8",
        "colab_type": "text"
      },
      "source": [
        "#### Requesting the StackExchange API for questions using the tags obatained\n",
        "1. A list of all the combination of tags is created to request the API.\n",
        "2. This is done to maximize the chance of getting questions with atleast any one of the tag included.\n",
        "3. All the question titles are stored into another list.\n",
        "4. The titles along with the API response is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I78zFp9YzoCz",
        "colab": {}
      },
      "source": [
        "#Requesting the StackExchange API for questions using the tags obatained\n",
        "def get_questions(tags):\n",
        "  temp = []\n",
        "  #Creating a list of all the possible combinations of tags\n",
        "  for i in range(1, len(tags)+1):\n",
        "      comb = []\n",
        "      comb.append(list(combinations(tags, i)))\n",
        "      for j in range(0, len(comb[0])):\n",
        "          temp.append(list(comb[0][j]))\n",
        "  #Making API calls to all the possible URLs\n",
        "  desc = []\n",
        "  questions = []\n",
        "  for i in range(len(temp)-1, -1, -1):\n",
        "      url = ''\n",
        "      for j in temp[i]:\n",
        "          url += j + ';'\n",
        "      URL = f'https://api.stackexchange.com/2.2/questions?order=asc&sort=activity&tagged={quote(url)}&site=stackoverflow'\n",
        "      r = requests.get(url = URL)\n",
        "      data = r.json()\n",
        "      if len(data['items']) > 2 :\n",
        "        for item in data['items']:\n",
        "          desc.append(item)\n",
        "          questions.append(item['title'])\n",
        "        break\n",
        "    \n",
        "  return [questions,desc]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smiblWybi5iD",
        "colab_type": "text"
      },
      "source": [
        "#### Calculating the similarities\n",
        "1. The `query` string is appended to the `questions` list.\n",
        "2. TensorFlow Hub's `universal-sentence-encoder-large/3` is used to calculate embeddings for all the questions.\n",
        "3. Then the inner product between all the question embeddings and the `query` string is calculated to get the similarity.\n",
        "4. The `probability` is added to the dictionary of question titles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LpTs8MFa0TdI",
        "colab": {}
      },
      "source": [
        "#Converting sentences to embeddings and computing the inner product to calculate similarity\n",
        "def get_similarity(questions, query):\n",
        "  questions.append(query)\n",
        "  with tf.Graph().as_default():\n",
        "    # Downloading the pre-trained \"Universal Sentence Encoder\" from tensorflow hub\n",
        "    url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
        "    embed = hub.Module(url)\n",
        "    question_encodings = embed(questions)\n",
        "    with tf.Session() as session:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        session.run(tf.tables_initializer())\n",
        "        embeddings = session.run(question_encodings)\n",
        "        similarity = np.inner(embeddings, embeddings[-1:])\n",
        "\n",
        "  # Adding probability values to the questions\n",
        "  dictItems = []\n",
        "  i = 0\n",
        "  for i in range(0, len(similarity)-1 ):\n",
        "      temp = { \"probability\" : similarity.item(i), \"title\" : questions[i] }\n",
        "      dictItems.append(temp)\n",
        "  return dictItems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJC8uZWpi5iI",
        "colab_type": "text"
      },
      "source": [
        "### Note:  \n",
        "Beautification tasks are handled by the frontend (Made using React.js).\n",
        "\n",
        "## Below is the example use of the code\n",
        "(Read README.md for using the full app)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6oqDQwTi5iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reduce logging\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "#user query string\n",
        "query = \"how to print string in c++\"\n",
        "\n",
        "#getting the tags from the query\n",
        "tags = get_tags(query)\n",
        "print(\"tags: \",tags)\n",
        "\n",
        "#getting the questions for the obtained tags\n",
        "questions = get_questions(tags)\n",
        "#the function returns a list. the 0th elements contains the list of questions\n",
        "print(\"list of question:\\n\", questions[0])\n",
        "\n",
        "#getting the similarity between the questions and query\n",
        "similarity = get_similarity(questions[0], query)\n",
        "print(\"Similarities:\\n\", similarity)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}