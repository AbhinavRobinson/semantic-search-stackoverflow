{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackOverflow Search Optimazation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4NDfFcheOu0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the input query to get tags\n",
    "This function uses NLTK (Natural language toolkit) to tokenize the query an d remove stopwords. A maximum of only 5 tags are returned as per the StackExchange API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkG4-QnazgE9"
   },
   "outputs": [],
   "source": [
    "def get_tags(input):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    tokenized_word=word_tokenize(input)\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_sent=[]\n",
    "    for w in tokenized_word:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    if len(filtered_sent) > 5:\n",
    "      return filtered_sent[:5]\n",
    "    else:\n",
    "      return filtered_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requesting the StackExchange API for questions using the tags obatained\n",
    "A list of all the combination of tags is created to request the API. This is done to maximize the chance of getting questions with atleast any one of the tag included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I78zFp9YzoCz"
   },
   "outputs": [],
   "source": [
    "def get_questions(tags):\n",
    "    temp = []\n",
    "    messages = []\n",
    "    data = []\n",
    "    #Creating a list of all the possible combinations of tags\n",
    "    for i in range(1, len(tags)+1):\n",
    "        comb = []\n",
    "        comb.append(list(combinations(tags, i)))\n",
    "        for j in range(0, len(comb[0])):\n",
    "            temp.append(list(comb[0][j]))\n",
    "    #Making API calls to all the possible URLs\n",
    "    messages = []\n",
    "    desc = []\n",
    "    for i in range(len(temp)-1, -1, -1):\n",
    "        url = ''\n",
    "        for j in temp[i]:\n",
    "            url += j + '%3B'\n",
    "        URL = f'https://api.stackexchange.com/2.2/questions?order=asc&sort=activity&tagged={url}&site=stackoverflow'\n",
    "        r = requests.get(url = URL)\n",
    "        data = r.json()\n",
    "        for item in data['items']:\n",
    "          desc.append(item)\n",
    "          messages.append(item['title'])\n",
    "    return [messages,desc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the similarities\n",
    "Using TensorFlow Hub's Universal Sentence Encoder to calculate embeddings for all the questions. Then the inner product of the embeddings gives the similarities between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpTs8MFa0TdI"
   },
   "outputs": [],
   "source": [
    "#Converting sentences to embeddings and computing the inner product to calculate similarity\n",
    "def get_similarity(questions):\n",
    "    url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
    "    embed = hub.Module(url)\n",
    "    placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "    question_encodings = embed(placeholder)\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        embeddings = session.run(question_encodings, feed_dict={placeholder: questions})\n",
    "        similarity = np.inner(embeddings, embeddings[-1:])\n",
    "    dictItems = []\n",
    "    i = 0\n",
    "    for i in range(0, len(similarity)-1 ):\n",
    "        temp = { \"probability\" : similarity.item(i), \"title\" : questions[i] }\n",
    "        dictItems.append(temp)\n",
    "    return dictItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Execution:\n",
    "1. `input` is defined.\n",
    "2. tags are obtained by calling `get_tags()`\n",
    "3. questions are obtained by calling `get_question()`\n",
    "4. Finally `get_similarity()` is called to calculate similarities.\n",
    "\n",
    "#### Note:\n",
    "* Sorting is taken care of in the frontend.\n",
    "* Error handling is also handled by the frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"define numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = get_tags(input)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = get_questions(tags)\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = get_similarity(questions[0])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
